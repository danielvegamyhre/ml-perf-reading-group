# Session 2: Flash Attention

In session 2 we covered the paper [FlashAttention: Fast and Memory-Efficient Exact Attention
with IO-Awareness](https://arxiv.org/pdf/2205.14135), and walked through my Triton kernel implementation.

**Presenters**: Ben Schneider, Daniel Vega-Myhre

### Links
- [Recording](https://youtu.be/Lys0TpsLIEc?si=T1Fy8Lf874Ax0d6S)
- [Triton kernel](./flash_attention.py)
- [Annotated flash attention paper](./flash_attention_[annotated].pdf)