# Session 11: Async Tensor Parallelism

EleutherAI ML Scalability & Performance Reading Group Session 11 meeting recording, where we covered the paper  "Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models," as well as how it is implemented in PyTorch as "async tensor parallelism."

Presenter: Daniel Vega-Myhre

### Links:
- [Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models](./overlapping-communication-and-computation-via-decomposition.pdf)
- [PyTorch Async Tensor Parallelism blog post](https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487)
- [Meeting recording](https://youtu.be/Ow1FLKFcSPs?si=KANmECRX1vRt4C_W)
- [Presentation diagrams](./diagrams.png)
