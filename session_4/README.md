# Session 4: Ring Attention with Blockwise Transformers for Near-Infinite Context Length

EleutherAI ML Scalability & Performance Reading Group Session 4 recording, in which we covered the seminal Ring Attention paper. We also cover 2 key pieces of prior work which provide the foundation for ring attention, to understand what the limitations were of those prior approaches and how ring attention built on them to unlock massive gains in max sequence length for transformer models.

**Presenter**: Daniel Vega-Myhre

Papers:
1. Sequence Parallelism: Long Sequence Training from System Perspective (https://aclanthology.org/2023.acl-long.134.pdf)
2. Blockwise Parallel Transformer for Large Context Models (https://arxiv.org/abs/2305.19370)
3. Ring Attention with Blockwise Transformers for Near-Infinite Context (https://arxiv.org/abs/2310.01889)

### Links:
- [Slides](https://docs.google.com/presentation/d/1mQQJf_aFqKUewRM97d0xAQQrGLmsnMQIjmO_dovYlWs/edit?usp=sharing)
- [Recording](https://youtu.be/fC9L8J7dVFI?si=3pfS2C3p0hTUDX5y)
- [Annotated sequence parallelism paper](./sequence_parallelism_[annotated].pdf)
- [Annotated blockwise transformers paper](./blockwise_transformers_[annotated].pdf)
- [Annotated ring attention paper](./ring_attention_[annotated].pdf)
