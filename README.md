# EleutherAI ML Scalability & Performance Reading Group

My annotated papers, slides, and meeting recordings for the EleutherAI ML Scalability & Performance research paper reading group.

Sessions:

1. [Session 1](./session_1/)
    - [Intro to GPU architecture, CUDA, NCCL, and common ML performance bottlenecks](https://www.youtube.com/watch?v=Cp7g1Ll4v0M)
2. [Session 2](./session_2/)
    - [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](./session_2/flash_attention_[annotated].pdf)
3. [Session 3](./session_3/)
    - [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
4. [Session 4](./session_4)
    - [Sequence Parallelism: Long Sequence Training from System Perspective](./session_4/sequence_parallelism_[annotated].pdf)
    - [Blockwise Parallel Transformer for Large Context Models](./session_4/blockwise_transformers_[annotated].pdf)
    - [Ring Attention with Blockwise Transformers for Near-Infinite Context Length](./session_4/ring_attention_[annotated].pdf)
5. [Session 5](./session_5/)
    - [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
6. [Session 6](./session_6)
    - [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](./session_6/gpipe_[annotated].pdf)
    - [PipeDream: Fast and Efficient Pipeline Parallel DNN Training](./session_6/pipe_dream_[annotated].pdf)
    - [Zero Bubble Pipeline Parallelism](./session_6/zero_bubble_pipeline_parallelism_[annotated].pdf)
7. [Session 7](./session_7/)
    - [DeepSeek V3](./session_7/deepseek-v3_[annotated].pdf)
    - [DeepSeek V2](./session_7/deepseek-v2_[annotated].pdf)
8. [Session 8](./session_8)
    - [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
9. [Session 9](./session_9/)
    - [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198)
10. [Session 10](./session_10/)
    - [Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression](https://arxiv.org/abs/2504.07389)
11. [Session 11](./session_11) 
    - [Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models](https://dl.acm.org/doi/abs/10.1145/3567955.3567959)
12. [Session 12](./session_12/)
    - [Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts](https://arxiv.org/pdf/2502.19811)
13. [Session 13](./session_13)
    - [Unified Sequence Parallelism](https://arxiv.org/abs/2405.07719)
14. [Session 14](./session_14/)
    - [A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630)