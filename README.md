# EleutherAI ML Scalability & Performance Reading Group

My annotated papers, slides, and meeting recordings for the EleutherAI ML Scalability & Performance research paper reading group.

Sessions:

1. [Session 1](./session_1/)
    - [Intro to GPU architecture, CUDA, NCCL, and common ML performance bottlenecks](https://www.youtube.com/watch?v=Cp7g1Ll4v0M)
2. [Session 2](./session_2/)
    - [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](./session_2/flash_attention_[annotated].pdf)
3. [Session 3](./session_3/)
    - [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
4. [Session 4](./session_4)
    - [Sequence Parallelism: Long Sequence Training from System Perspective](./session_4/sequence_parallelism_[annotated].pdf)
    - [Blockwise Parallel Transformer for Large Context Models](./session_4/blockwise_transformers_[annotated].pdf)
    - [Ring Attention with Blockwise Transformers for Near-Infinite Context Length](./session_4/ring_attention_[annotated].pdf)
5. [Session 5](./session_5/)
    - [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
6. [Session 6](./session_6)
    - [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](./session_6/gpipe_[annotated].pdf)
    - [PipeDream: Fast and Efficient Pipeline Parallel DNN Training](./session_6/pipe_dream_[annotated].pdf)
    - [Zero Bubble Pipeline Parallelism](./session_6/zero_bubble_pipeline_parallelism_[annotated].pdf)
7. [Session 7](./session_7/)
    - [DeepSeek V3](./session_7/deepseek-v3_[annotated].pdf)
    - [DeepSeek V2](./session_7/deepseek-v2_[annotated].pdf)
8. [Session 8](./session_8)
    - [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
9. [Session 9](./session_9/)
    - [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198)
10. [Session 10](./session_10/)
    - [Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression](https://arxiv.org/abs/2504.07389)